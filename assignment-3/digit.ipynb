{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from NeuralNetwork import Sequential, LinearLayer, SigmoidLayer, SoftmaxLayer, CrossEntropyLoss, TanhLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100000000\n",
    "learning_rate =  0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MNIST dataset\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "\n",
    "# Preprocess the data\n",
    "X_train = X_train.reshape((60000, 784)) / 255.0\n",
    "X_test = X_test.reshape((10000, 784)) / 255.0\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "# Create a validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.1, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: loss = 3.38818592550976\n",
      "Iteration 1: loss = 3.331281972414203\n",
      "Iteration 2: loss = 3.2990392932297246\n",
      "Iteration 3: loss = 3.276514877156767\n",
      "Iteration 4: loss = 3.2589216663943925\n",
      "Iteration 5: loss = 3.244234841846234\n",
      "Iteration 6: loss = 3.2314337463960316\n",
      "Iteration 7: loss = 3.2199285851841806\n",
      "Iteration 8: loss = 3.2093398085952773\n",
      "Iteration 9: loss = 3.1994052067233407\n",
      "Iteration 10: loss = 3.189936912269977\n",
      "Iteration 11: loss = 3.1807986170229103\n",
      "Iteration 12: loss = 3.1718917116209204\n",
      "Iteration 13: loss = 3.1631459579160373\n",
      "Iteration 14: loss = 3.154512735562167\n",
      "Iteration 15: loss = 3.145959822717173\n",
      "Iteration 16: loss = 3.137467131585526\n",
      "Iteration 17: loss = 3.1290231145069227\n",
      "Iteration 18: loss = 3.120621740873631\n",
      "Iteration 19: loss = 3.112260039489096\n",
      "Iteration 20: loss = 3.103936233602761\n",
      "Iteration 21: loss = 3.0956484873957453\n",
      "Iteration 22: loss = 3.087394238653608\n",
      "Iteration 23: loss = 3.0791700230732824\n",
      "Iteration 24: loss = 3.070971630643073\n",
      "Iteration 25: loss = 3.062794407767396\n",
      "Iteration 26: loss = 3.054633543800417\n",
      "Iteration 27: loss = 3.0464842437704855\n",
      "Iteration 28: loss = 3.038341762667392\n",
      "Iteration 29: loss = 3.030201335487589\n",
      "Iteration 30: loss = 3.0220580672323814\n",
      "Iteration 31: loss = 3.0139068467238035\n",
      "Iteration 32: loss = 3.005742325906385\n",
      "Iteration 33: loss = 2.9975589758303927\n",
      "Iteration 34: loss = 2.989351204579012\n",
      "Iteration 35: loss = 2.981113508396988\n",
      "Iteration 36: loss = 2.9728406258668914\n",
      "Iteration 37: loss = 2.9645276721342375\n",
      "Iteration 38: loss = 2.9561702405378996\n",
      "Iteration 39: loss = 2.9477644684131055\n",
      "Iteration 40: loss = 2.939307070289525\n",
      "Iteration 41: loss = 2.930795344974696\n",
      "Iteration 42: loss = 2.9222271638193793\n",
      "Iteration 43: loss = 2.91360094678046\n",
      "Iteration 44: loss = 2.904915631551839\n",
      "Iteration 45: loss = 2.8961706395699007\n",
      "Iteration 46: loss = 2.8873658414057153\n",
      "Iteration 47: loss = 2.878501523041562\n",
      "Iteration 48: loss = 2.8695783537999953\n",
      "Iteration 49: loss = 2.86059735620819\n",
      "Iteration 50: loss = 2.851559877781806\n",
      "Iteration 51: loss = 2.842467564548003\n",
      "Iteration 52: loss = 2.8333223360521713\n",
      "Iteration 53: loss = 2.8241263615749017\n",
      "Iteration 54: loss = 2.81488203730117\n",
      "Iteration 55: loss = 2.8055919642174625\n",
      "Iteration 56: loss = 2.7962589265542834\n",
      "Iteration 57: loss = 2.7868858706356594\n",
      "Iteration 58: loss = 2.777475884039729\n",
      "Iteration 59: loss = 2.768032175013886\n",
      "Iteration 60: loss = 2.758558052123003\n",
      "Iteration 61: loss = 2.7490569041396684\n",
      "Iteration 62: loss = 2.739532180211046\n",
      "Iteration 63: loss = 2.729987370358019\n",
      "Iteration 64: loss = 2.7204259863788103\n",
      "Iteration 65: loss = 2.7108515432415348\n",
      "Iteration 66: loss = 2.7012675410582796\n",
      "Iteration 67: loss = 2.6916774477377867\n",
      "Iteration 68: loss = 2.6820846824146094\n",
      "Iteration 69: loss = 2.672492599750335\n",
      "Iteration 70: loss = 2.662904475197383\n",
      "Iteration 71: loss = 2.6533234913082007\n",
      "Iteration 72: loss = 2.643752725163103\n",
      "Iteration 73: loss = 2.6341951369787733\n",
      "Iteration 74: loss = 2.6246535599469376\n",
      "Iteration 75: loss = 2.6151306913396084\n",
      "Iteration 76: loss = 2.605629084903685\n",
      "Iteration 77: loss = 2.5961511445542547\n",
      "Iteration 78: loss = 2.5866991193627284\n",
      "Iteration 79: loss = 2.577275099823536\n",
      "Iteration 80: loss = 2.567881015371503\n",
      "Iteration 81: loss = 2.558518633111546\n",
      "Iteration 82: loss = 2.5491895577130594\n",
      "Iteration 83: loss = 2.539895232413347\n",
      "Iteration 84: loss = 2.530636941067678\n",
      "Iteration 85: loss = 2.5214158111781337\n",
      "Iteration 86: loss = 2.5122328178289615\n",
      "Iteration 87: loss = 2.5030887884529736\n",
      "Iteration 88: loss = 2.4939844083511407\n",
      "Iteration 89: loss = 2.4849202268859654\n",
      "Iteration 90: loss = 2.4758966642684332\n",
      "Iteration 91: loss = 2.4669140188579504\n",
      "Iteration 92: loss = 2.457972474894874\n",
      "Iteration 93: loss = 2.449072110585771\n",
      "Iteration 94: loss = 2.440212906462394\n",
      "Iteration 95: loss = 2.4313947539365492\n",
      "Iteration 96: loss = 2.422617463974577\n",
      "Iteration 97: loss = 2.4138807758169554\n",
      "Iteration 98: loss = 2.4051843656708596\n",
      "Iteration 99: loss = 2.3965278553061817\n",
      "Iteration 100: loss = 2.3879108204887185\n",
      "Iteration 101: loss = 2.3793327991880164\n",
      "Iteration 102: loss = 2.3707932995016496\n",
      "Iteration 103: loss = 2.362291807242645\n",
      "Iteration 104: loss = 2.353827793142226\n",
      "Iteration 105: loss = 2.3454007196259563\n",
      "Iteration 106: loss = 2.337010047127824\n",
      "Iteration 107: loss = 2.3286552399133784\n",
      "Iteration 108: loss = 2.32033577138996\n",
      "Iteration 109: loss = 2.3120511288888586\n",
      "Iteration 110: loss = 2.303800817910937\n",
      "Iteration 111: loss = 2.2955843658336508\n",
      "Iteration 112: loss = 2.2874013250832683\n",
      "Iteration 113: loss = 2.2792512757813927\n",
      "Iteration 114: loss = 2.271133827879446\n",
      "Iteration 115: loss = 2.26304862279856\n",
      "Iteration 116: loss = 2.2549953345952183\n",
      "Iteration 117: loss = 2.2469736706751857\n",
      "Iteration 118: loss = 2.238983372079639\n",
      "Iteration 119: loss = 2.2310242133682228\n",
      "Iteration 120: loss = 2.2230960021241315\n",
      "Iteration 121: loss = 2.2151985781063828\n",
      "Iteration 122: loss = 2.2073318120745737\n",
      "Iteration 123: loss = 2.199495604311549\n",
      "Iteration 124: loss = 2.191689882869976\n",
      "Iteration 125: loss = 2.1839146015696955\n",
      "Iteration 126: loss = 2.176169737774053\n",
      "Iteration 127: loss = 2.1684552899751712\n",
      "Iteration 128: loss = 2.1607712752200157\n",
      "Iteration 129: loss = 2.153117726411198\n",
      "Iteration 130: loss = 2.1454946895183107\n",
      "Iteration 131: loss = 2.1379022207370775\n",
      "Iteration 132: loss = 2.130340383634539\n",
      "Iteration 133: loss = 2.122809246318652\n",
      "Iteration 134: loss = 2.115308878670049\n",
      "Iteration 135: loss = 2.1078393496721013\n",
      "Iteration 136: loss = 2.1004007248730305\n",
      "Iteration 137: loss = 2.092993064010646\n",
      "Iteration 138: loss = 2.0856164188263646\n",
      "Iteration 139: loss = 2.078270831090903\n",
      "Iteration 140: loss = 2.0709563308593713\n",
      "Iteration 141: loss = 2.0636729349688556\n",
      "Iteration 142: loss = 2.0564206457869965\n",
      "Iteration 143: loss = 2.049199450215869\n",
      "Iteration 144: loss = 2.0420093189516715\n",
      "Iteration 145: loss = 2.0348502059974467\n",
      "Iteration 146: loss = 2.027722048423369\n",
      "Iteration 147: loss = 2.0206247663668466\n",
      "Iteration 148: loss = 2.0135582632629245\n",
      "Iteration 149: loss = 2.0065224262939525\n",
      "Iteration 150: loss = 1.9995171270462084\n",
      "Iteration 151: loss = 1.99254222236002\n",
      "Iteration 152: loss = 1.9855975553588086\n",
      "Iteration 153: loss = 1.9786829566414272\n",
      "Iteration 154: loss = 1.9717982456210257\n",
      "Iteration 155: loss = 1.9649432319926565\n",
      "Iteration 156: loss = 1.9581177173107611\n",
      "Iteration 157: loss = 1.9513214966566692\n",
      "Iteration 158: loss = 1.9445543603753854\n",
      "Iteration 159: loss = 1.9378160958601622\n",
      "Iteration 160: loss = 1.9311064893627212\n",
      "Iteration 161: loss = 1.9244253278066292\n",
      "Iteration 162: loss = 1.9177724005810854\n",
      "Iteration 163: loss = 1.9111475012925245\n",
      "Iteration 164: loss = 1.9045504294517903\n",
      "Iteration 165: loss = 1.8979809920753605\n",
      "Iteration 166: loss = 1.891439005180267\n",
      "Iteration 167: loss = 1.8849242951537977\n",
      "Iteration 168: loss = 1.8784366999810873\n",
      "Iteration 169: loss = 1.871976070316007\n",
      "Iteration 170: loss = 1.8655422703835707\n",
      "Iteration 171: loss = 1.8591351787051913\n",
      "Iteration 172: loss = 1.8527546886415716\n",
      "Iteration 173: loss = 1.8464007087516263\n",
      "Iteration 174: loss = 1.8400731629696163\n",
      "Iteration 175: loss = 1.833771990606378\n",
      "Iteration 176: loss = 1.827497146184135\n",
      "Iteration 177: loss = 1.8212485991176703\n",
      "Iteration 178: loss = 1.815026333257553\n",
      "Iteration 179: loss = 1.808830346313465\n",
      "Iteration 180: loss = 1.8026606491775043\n",
      "Iteration 181: loss = 1.7965172651683743\n",
      "Iteration 182: loss = 1.7904002292178247\n",
      "Iteration 183: loss = 1.7843095870203447\n",
      "Iteration 184: loss = 1.778245394166138\n",
      "Iteration 185: loss = 1.7722077152757263\n",
      "Iteration 186: loss = 1.7661966231523256\n",
      "Iteration 187: loss = 1.7602121979654042\n",
      "Iteration 188: loss = 1.7542545264757288\n",
      "Iteration 189: loss = 1.748323701308783\n",
      "Iteration 190: loss = 1.7424198202798526\n",
      "Iteration 191: loss = 1.7365429857704786\n",
      "Iteration 192: loss = 1.7306933041523689\n",
      "Iteration 193: loss = 1.7248708852516885\n",
      "Iteration 194: loss = 1.7190758418436858\n",
      "Iteration 195: loss = 1.7133082891654385\n",
      "Iteration 196: loss = 1.7075683444329455\n",
      "Iteration 197: loss = 1.7018561263481524\n",
      "Iteration 198: loss = 1.696171754581803\n",
      "Iteration 199: loss = 1.6905153492193314\n",
      "Iteration 200: loss = 1.6848870301591874\n",
      "Iteration 201: loss = 1.6792869164561037\n",
      "Iteration 202: loss = 1.6737151256055125\n",
      "Iteration 203: loss = 1.668171772769544\n",
      "Iteration 204: loss = 1.662656969949365\n",
      "Iteration 205: loss = 1.6571708251129609\n",
      "Iteration 206: loss = 1.6517134412913559\n",
      "Iteration 207: loss = 1.646284915659581\n",
      "Iteration 208: loss = 1.640885338621153\n",
      "Iteration 209: loss = 1.6355147929162346\n",
      "Iteration 210: loss = 1.6301733527739803\n",
      "Iteration 211: loss = 1.62486108312869\n",
      "Iteration 212: loss = 1.6195780389175385\n",
      "Iteration 213: loss = 1.6143242644748144\n",
      "Iteration 214: loss = 1.6090997930341002\n",
      "Iteration 215: loss = 1.6039046463459523\n",
      "Iteration 216: loss = 1.5987388344146087\n",
      "Iteration 217: loss = 1.5936023553534293\n",
      "Iteration 218: loss = 1.588495195355337\n",
      "Iteration 219: loss = 1.5834173287717146\n",
      "Iteration 220: loss = 1.5783687182910549\n",
      "Iteration 221: loss = 1.5733493152072755\n",
      "Iteration 222: loss = 1.5683590597668806\n",
      "Iteration 223: loss = 1.5633978815840277\n",
      "Iteration 224: loss = 1.5584657001129618\n",
      "Iteration 225: loss = 1.5535624251679603\n",
      "Iteration 226: loss = 1.54868795748191\n",
      "Iteration 227: loss = 1.5438421892956578\n",
      "Iteration 228: loss = 1.5390250049713532\n",
      "Iteration 229: loss = 1.5342362816239337\n",
      "Iteration 230: loss = 1.5294758897658145\n",
      "Iteration 231: loss = 1.5247436939604824\n",
      "Iteration 232: loss = 1.5200395534812514\n",
      "Iteration 233: loss = 1.5153633229717915\n",
      "Iteration 234: loss = 1.5107148531052392\n",
      "Iteration 235: loss = 1.50609399123879\n",
      "Iteration 236: loss = 1.5015005820606684\n",
      "Iteration 237: loss = 1.4969344682262875\n",
      "Iteration 238: loss = 1.4923954909803372\n",
      "Iteration 239: loss = 1.4878834907614642\n",
      "Iteration 240: loss = 1.483398307786189\n",
      "Iteration 241: loss = 1.4789397826087374\n",
      "Iteration 242: loss = 1.4745077566536233\n",
      "Iteration 243: loss = 1.4701020727180463\n",
      "Iteration 244: loss = 1.4657225754414744\n",
      "Iteration 245: loss = 1.4613691117402479\n",
      "Iteration 246: loss = 1.4570415312054368\n",
      "Iteration 247: loss = 1.4527396864627593\n",
      "Iteration 248: loss = 1.4484634334938165\n",
      "Iteration 249: loss = 1.444212631918415\n",
      "Iteration 250: loss = 1.439987145238112\n",
      "Iteration 251: loss = 1.435786841041462\n",
      "Iteration 252: loss = 1.4316115911716112\n",
      "Iteration 253: loss = 1.427461271856966\n",
      "Iteration 254: loss = 1.4233357638056139\n",
      "Iteration 255: loss = 1.4192349522639585\n",
      "Iteration 256: loss = 1.4151587270398298\n",
      "Iteration 257: loss = 1.4111069824899514\n",
      "Iteration 258: loss = 1.4070796174713385\n",
      "Iteration 259: loss = 1.4030765352559056\n",
      "Iteration 260: loss = 1.3990976434072961\n",
      "Iteration 261: loss = 1.3951428536188677\n",
      "Iteration 262: loss = 1.391212081511794\n",
      "Iteration 263: loss = 1.387305246392459\n",
      "Iteration 264: loss = 1.3834222709687376\n",
      "Iteration 265: loss = 1.3795630810253277\n",
      "Iteration 266: loss = 1.3757276050590903\n",
      "Iteration 267: loss = 1.3719157738761845\n",
      "Iteration 268: loss = 1.3681275201538248\n",
      "Iteration 269: loss = 1.3643627779704264\n",
      "Iteration 270: loss = 1.360621482308913\n",
      "Iteration 271: loss = 1.3569035685388493\n",
      "Iteration 272: loss = 1.3532089718837725\n",
      "Iteration 273: loss = 1.3495376268807517\n",
      "Iteration 274: loss = 1.3458894668394865\n",
      "Iteration 275: loss = 1.3422644233085086\n",
      "Iteration 276: loss = 1.3386624255559483\n",
      "Iteration 277: loss = 1.3350834000721026\n",
      "Iteration 278: loss = 1.331527270100603\n",
      "Iteration 279: loss = 1.3279939552043654\n",
      "Iteration 280: loss = 1.3244833708717976\n",
      "Iteration 281: loss = 1.320995428167833\n",
      "Iteration 282: loss = 1.3175300334334463\n",
      "Iteration 283: loss = 1.3140870880362525\n",
      "Iteration 284: loss = 1.3106664881737793\n",
      "Iteration 285: loss = 1.3072681247298839\n",
      "Iteration 286: loss = 1.3038918831837907\n",
      "Iteration 287: loss = 1.3005376435702194\n",
      "Iteration 288: loss = 1.2972052804881664\n",
      "Iteration 289: loss = 1.29389466315514\n",
      "Iteration 290: loss = 1.2906056555029675\n",
      "Iteration 291: loss = 1.2873381163108126\n",
      "Iteration 292: loss = 1.284091899370693\n",
      "Iteration 293: loss = 1.2808668536806087\n",
      "Iteration 294: loss = 1.2776628236603547\n",
      "Iteration 295: loss = 1.274479649385229\n",
      "Iteration 296: loss = 1.271317166833034\n",
      "Iteration 297: loss = 1.2681752081401259\n",
      "Iteration 298: loss = 1.2650536018626262\n",
      "Iteration 299: loss = 1.2619521732393757\n",
      "Iteration 300: loss = 1.2588707444536194\n",
      "Iteration 301: loss = 1.2558091348909242\n",
      "Iteration 302: loss = 1.2527671613912554\n",
      "Iteration 303: loss = 1.2497446384935738\n",
      "Iteration 304: loss = 1.2467413786717458\n",
      "Iteration 305: loss = 1.243757192560939\n",
      "Iteration 306: loss = 1.2407918891740244\n",
      "Iteration 307: loss = 1.2378452761078393\n",
      "Iteration 308: loss = 1.2349171597394442\n",
      "Iteration 309: loss = 1.232007345412741\n",
      "Iteration 310: loss = 1.229115637616025\n",
      "Iteration 311: loss = 1.2262418401511779\n",
      "Iteration 312: loss = 1.2233857562952857\n",
      "Iteration 313: loss = 1.220547188955521\n",
      "Iteration 314: loss = 1.217725940818083\n",
      "Iteration 315: loss = 1.2149218144919347\n",
      "Iteration 316: loss = 1.212134612647972\n",
      "Iteration 317: loss = 1.20936413815411\n",
      "Iteration 318: loss = 1.2066101942066458\n",
      "Iteration 319: loss = 1.2038725844580835\n",
      "Iteration 320: loss = 1.201151113141473\n",
      "Iteration 321: loss = 1.198445585191188\n",
      "Iteration 322: loss = 1.1957558063599778\n",
      "Iteration 323: loss = 1.1930815833320387\n",
      "Iteration 324: loss = 1.190422723831833\n",
      "Iteration 325: loss = 1.1877790367283667\n",
      "Iteration 326: loss = 1.1851503321346624\n",
      "Iteration 327: loss = 1.1825364215022034\n",
      "Iteration 328: loss = 1.179937117710168\n",
      "Iteration 329: loss = 1.1773522351493737\n",
      "Iteration 330: loss = 1.174781589800886\n",
      "Iteration 331: loss = 1.1722249993093246\n",
      "Iteration 332: loss = 1.1696822830509868\n",
      "Iteration 333: loss = 1.1671532621969245\n",
      "Iteration 334: loss = 1.164637759771175\n",
      "Iteration 335: loss = 1.1621356007043944\n",
      "Iteration 336: loss = 1.1596466118831348\n",
      "Iteration 337: loss = 1.157170622195027\n",
      "Iteration 338: loss = 1.1547074625701652\n",
      "Iteration 339: loss = 1.1522569660189386\n",
      "Iteration 340: loss = 1.1498189676665869\n",
      "Iteration 341: loss = 1.1473933047847262\n",
      "Iteration 342: loss = 1.1449798168201\n",
      "Iteration 343: loss = 1.1425783454207634\n",
      "Iteration 344: loss = 1.1401887344599457\n",
      "Iteration 345: loss = 1.1378108300577836\n",
      "Iteration 346: loss = 1.1354444806011426\n",
      "Iteration 347: loss = 1.13308953676172\n",
      "Iteration 348: loss = 1.1307458515126236\n",
      "Iteration 349: loss = 1.1284132801436206\n",
      "Iteration 350: loss = 1.126091680275252\n",
      "Iteration 351: loss = 1.1237809118719704\n",
      "Iteration 352: loss = 1.1214808372545233\n",
      "Iteration 353: loss = 1.1191913211117053\n",
      "Iteration 354: loss = 1.1169122305116792\n",
      "Iteration 355: loss = 1.114643434912996\n",
      "Iteration 356: loss = 1.1123848061754478\n",
      "Iteration 357: loss = 1.1101362185708896\n",
      "Iteration 358: loss = 1.1078975487941136\n",
      "Iteration 359: loss = 1.1056686759738716\n",
      "Iteration 360: loss = 1.1034494816841036\n",
      "Iteration 361: loss = 1.1012398499554277\n",
      "Iteration 362: loss = 1.0990396672869152\n",
      "Iteration 363: loss = 1.096848822658164\n",
      "Iteration 364: loss = 1.0946672075416892\n",
      "Iteration 365: loss = 1.0924947159155873\n",
      "Iteration 366: loss = 1.0903312442764914\n",
      "Iteration 367: loss = 1.088176691652748\n",
      "Iteration 368: loss = 1.0860309596177973\n",
      "Iteration 369: loss = 1.0838939523036841\n",
      "Iteration 370: loss = 1.0817655764146277\n",
      "Iteration 371: loss = 1.0796457412405551\n",
      "Iteration 372: loss = 1.0775343586704809\n",
      "Iteration 373: loss = 1.0754313432055673\n",
      "Iteration 374: loss = 1.0733366119716832\n",
      "Iteration 375: loss = 1.0712500847312176\n",
      "Iteration 376: loss = 1.0691716838938572\n",
      "Iteration 377: loss = 1.0671013345259952\n",
      "Iteration 378: loss = 1.0650389643583646\n",
      "Iteration 379: loss = 1.0629845037914578\n",
      "Iteration 380: loss = 1.0609378858982261\n",
      "Iteration 381: loss = 1.0588990464235346\n",
      "Iteration 382: loss = 1.0568679237798024\n",
      "Iteration 383: loss = 1.0548444590382537\n",
      "Iteration 384: loss = 1.052828595915228\n",
      "Iteration 385: loss = 1.0508202807529832\n",
      "Iteration 386: loss = 1.0488194624945284\n",
      "Iteration 387: loss = 1.046826092652028\n",
      "Iteration 388: loss = 1.0448401252684603\n",
      "Iteration 389: loss = 1.042861516872268\n",
      "Iteration 390: loss = 1.0408902264248907\n",
      "Iteration 391: loss = 1.0389262152611594\n",
      "Iteration 392: loss = 1.0369694470226642\n",
      "Iteration 393: loss = 1.0350198875843617\n",
      "Iteration 394: loss = 1.0330775049747514\n",
      "Iteration 395: loss = 1.0311422692901162\n",
      "Iteration 396: loss = 1.0292141526033933\n",
      "Iteration 397: loss = 1.0272931288683262\n",
      "Iteration 398: loss = 1.0253791738196527\n",
      "Iteration 399: loss = 1.0234722648700987\n",
      "Iteration 400: loss = 1.0215723810050366\n",
      "Iteration 401: loss = 1.0196795026756618\n",
      "Iteration 402: loss = 1.0177936116915953\n",
      "Iteration 403: loss = 1.0159146911137722\n",
      "Iteration 404: loss = 1.014042725148533\n",
      "Iteration 405: loss = 1.0121776990437494\n",
      "Iteration 406: loss = 1.0103195989878186\n",
      "Iteration 407: loss = 1.0084684120122924\n",
      "Iteration 408: loss = 1.0066241258988506\n",
      "Iteration 409: loss = 1.0047867290912378\n",
      "Iteration 410: loss = 1.0029562106127041\n",
      "Iteration 411: loss = 1.001132559989374\n",
      "Iteration 412: loss = 0.9993157671798659\n",
      "Iteration 413: loss = 0.9975058225113559\n",
      "Iteration 414: loss = 0.9957027166221737\n",
      "Iteration 415: loss = 0.9939064404108664\n",
      "Iteration 416: loss = 0.9921169849916105\n",
      "Iteration 417: loss = 0.9903343416556827\n",
      "Iteration 418: loss = 0.988558501838664\n",
      "Iteration 419: loss = 0.986789457092952\n",
      "Iteration 420: loss = 0.9850271990651168\n",
      "Iteration 421: loss = 0.983271719477584\n",
      "Iteration 422: loss = 0.9815230101141459\n",
      "Iteration 423: loss = 0.9797810628087698\n",
      "Iteration 424: loss = 0.978045869437226\n",
      "Iteration 425: loss = 0.9763174219110743\n",
      "Iteration 426: loss = 0.9745957121735955\n",
      "Iteration 427: loss = 0.9728807321973303\n",
      "Iteration 428: loss = 0.9711724739828981\n",
      "Iteration 429: loss = 0.9694709295588945\n",
      "Iteration 430: loss = 0.9677760909826438\n",
      "Iteration 431: loss = 0.9660879503417086\n",
      "Iteration 432: loss = 0.9644064997560388\n",
      "Iteration 433: loss = 0.9627317313807195\n",
      "Iteration 434: loss = 0.9610636374092808\n",
      "Iteration 435: loss = 0.9594022100775361\n",
      "Iteration 436: loss = 0.957747441667957\n",
      "Iteration 437: loss = 0.9560993245145313\n",
      "Iteration 438: loss = 0.9544578510080798\n",
      "Iteration 439: loss = 0.9528230136019596\n",
      "Iteration 440: loss = 0.9511948048180473\n",
      "Iteration 441: loss = 0.9495732172528881\n",
      "Iteration 442: loss = 0.9479582435838252\n",
      "Iteration 443: loss = 0.9463498765749206\n",
      "Iteration 444: loss = 0.9447481090824269\n",
      "Iteration 445: loss = 0.9431529340595496\n",
      "Iteration 446: loss = 0.9415643445602355\n",
      "Iteration 447: loss = 0.9399823337416883\n",
      "Iteration 448: loss = 0.9384068948653392\n",
      "Iteration 449: loss = 0.9368380212960081\n",
      "Iteration 450: loss = 0.9352757064990083\n",
      "Iteration 451: loss = 0.933719944034994\n",
      "Iteration 452: loss = 0.9321707275523967\n",
      "Iteration 453: loss = 0.9306280507773345\n",
      "Iteration 454: loss = 0.9290919075009584\n",
      "Iteration 455: loss = 0.927562291564231\n",
      "Iteration 456: loss = 0.9260391968402232\n",
      "Iteration 457: loss = 0.9245226172140348\n",
      "Iteration 458: loss = 0.9230125465605217\n",
      "Iteration 459: loss = 0.9215089787200361\n",
      "Iteration 460: loss = 0.9200119074724296\n",
      "Iteration 461: loss = 0.9185213265095841\n",
      "Iteration 462: loss = 0.9170372294067516\n",
      "Iteration 463: loss = 0.9155596095930018\n",
      "Iteration 464: loss = 0.914088460321058\n",
      "Iteration 465: loss = 0.9126237746368103\n",
      "Iteration 466: loss = 0.9111655453487851\n",
      "Iteration 467: loss = 0.9097137649978233\n",
      "Iteration 468: loss = 0.9082684258272357\n",
      "Iteration 469: loss = 0.9068295197536599\n",
      "Iteration 470: loss = 0.905397038338851\n",
      "Iteration 471: loss = 0.9039709727626283\n",
      "Iteration 472: loss = 0.9025513137971588\n",
      "Iteration 473: loss = 0.9011380517828003\n",
      "Iteration 474: loss = 0.8997311766056485\n",
      "Iteration 475: loss = 0.8983306776769712\n",
      "Iteration 476: loss = 0.8969365439146678\n",
      "Iteration 477: loss = 0.8955487637268716\n",
      "Iteration 478: loss = 0.8941673249978063\n",
      "Iteration 479: loss = 0.8927922150759521\n",
      "Iteration 480: loss = 0.8914234207645847\n",
      "Iteration 481: loss = 0.8900609283146803\n",
      "Iteration 482: loss = 0.8887047234201714\n",
      "Iteration 483: loss = 0.8873547912155083\n",
      "Iteration 484: loss = 0.8860111162754192\n",
      "Iteration 485: loss = 0.884673682616772\n",
      "Iteration 486: loss = 0.8833424737023866\n",
      "Iteration 487: loss = 0.8820174724466311\n",
      "Iteration 488: loss = 0.8806986612226231\n",
      "Iteration 489: loss = 0.8793860218708405\n",
      "Iteration 490: loss = 0.8780795357089353\n",
      "Iteration 491: loss = 0.8767791835425516\n",
      "Iteration 492: loss = 0.8754849456769481\n",
      "Iteration 493: loss = 0.8741968019292292\n",
      "Iteration 494: loss = 0.8729147316410154\n",
      "Iteration 495: loss = 0.871638713691386\n",
      "Iteration 496: loss = 0.8703687265099604\n",
      "Iteration 497: loss = 0.8691047480899977\n",
      "Iteration 498: loss = 0.8678467560014278\n",
      "Iteration 499: loss = 0.8665947274037357\n",
      "Iteration 500: loss = 0.865348639058678\n",
      "Iteration 501: loss = 0.8641084673427877\n",
      "Iteration 502: loss = 0.8628741882596934\n",
      "Iteration 503: loss = 0.8616457774522619\n",
      "Iteration 504: loss = 0.8604232102146141\n",
      "Iteration 505: loss = 0.8592064615040581\n",
      "Iteration 506: loss = 0.8579955059530107\n",
      "Iteration 507: loss = 0.8567903178809634\n",
      "Iteration 508: loss = 0.855590871306568\n",
      "Iteration 509: loss = 0.8543971399598965\n",
      "Iteration 510: loss = 0.8532090972949382\n",
      "Iteration 511: loss = 0.8520267165023697\n",
      "Iteration 512: loss = 0.8508499705226333\n",
      "Iteration 513: loss = 0.8496788320593366\n",
      "Iteration 514: loss = 0.8485132735929721\n",
      "Iteration 515: loss = 0.8473532673949363\n",
      "Iteration 516: loss = 0.8461987855418129\n",
      "Iteration 517: loss = 0.845049799929874\n",
      "Iteration 518: loss = 0.843906282289727\n",
      "Iteration 519: loss = 0.8427682042010426\n",
      "Iteration 520: loss = 0.8416355371072726\n",
      "Iteration 521: loss = 0.8405082523302763\n",
      "Iteration 522: loss = 0.8393863210847607\n",
      "Iteration 523: loss = 0.8382697144924498\n",
      "Iteration 524: loss = 0.8371584035958941\n",
      "Iteration 525: loss = 0.836052359371841\n",
      "Iteration 526: loss = 0.8349515527440933\n",
      "Iteration 527: loss = 0.8338559545957938\n",
      "Iteration 528: loss = 0.8327655357810784\n",
      "Iteration 529: loss = 0.8316802671360597\n",
      "Iteration 530: loss = 0.8306001194890985\n",
      "Iteration 531: loss = 0.8295250636703551\n",
      "Iteration 532: loss = 0.8284550705205919\n",
      "Iteration 533: loss = 0.827390110899237\n",
      "Iteration 534: loss = 0.8263301556917023\n",
      "Iteration 535: loss = 0.82527517581598\n",
      "Iteration 536: loss = 0.8242251422285222\n",
      "Iteration 537: loss = 0.8231800259294431\n",
      "Iteration 538: loss = 0.8221397979670546\n",
      "Iteration 539: loss = 0.8211044294417867\n",
      "Iteration 540: loss = 0.8200738915095105\n",
      "Iteration 541: loss = 0.8190481553843105\n",
      "Iteration 542: loss = 0.8180271923407477\n",
      "Iteration 543: loss = 0.8170109737156445\n",
      "Iteration 544: loss = 0.8159994709094442\n",
      "Iteration 545: loss = 0.8149926553871835\n",
      "Iteration 546: loss = 0.813990498679118\n",
      "Iteration 547: loss = 0.8129929723810566\n",
      "Iteration 548: loss = 0.8120000481544394\n",
      "Iteration 549: loss = 0.811011697726209\n",
      "Iteration 550: loss = 0.8100278928885289\n",
      "Iteration 551: loss = 0.8090486054983812\n",
      "Iteration 552: loss = 0.8080738074771054\n",
      "Iteration 553: loss = 0.8071034708099187\n",
      "Iteration 554: loss = 0.806137567545464\n",
      "Iteration 555: loss = 0.8051760697954344\n",
      "Iteration 556: loss = 0.8042189497343141\n",
      "Iteration 557: loss = 0.8032661795992861\n",
      "Iteration 558: loss = 0.8023177316903404\n",
      "Iteration 559: loss = 0.8013735783706217\n",
      "Iteration 560: loss = 0.8004336920670584\n",
      "Iteration 561: loss = 0.7994980452712888\n",
      "Iteration 562: loss = 0.7985666105409256\n",
      "Iteration 563: loss = 0.7976393605011729\n",
      "Iteration 564: loss = 0.7967162678468089\n",
      "Iteration 565: loss = 0.7957973053445525\n",
      "Iteration 566: loss = 0.7948824458358098\n",
      "Iteration 567: loss = 0.7939716622398045\n",
      "Iteration 568: loss = 0.793064927557087\n",
      "Iteration 569: loss = 0.792162214873395\n",
      "Iteration 570: loss = 0.7912634973638687\n",
      "Iteration 571: loss = 0.790368748297571\n",
      "Iteration 572: loss = 0.7894779410423077\n",
      "Iteration 573: loss = 0.7885910490696892\n",
      "Iteration 574: loss = 0.7877080459604152\n",
      "Iteration 575: loss = 0.786828905409728\n",
      "Iteration 576: loss = 0.7859536012329955\n",
      "Iteration 577: loss = 0.7850821073713745\n",
      "Iteration 578: loss = 0.7842143978975097\n",
      "Iteration 579: loss = 0.7833504470212114\n",
      "Iteration 580: loss = 0.7824902290950799\n",
      "Iteration 581: loss = 0.7816337186200087\n",
      "Iteration 582: loss = 0.7807808902505377\n",
      "Iteration 583: loss = 0.7799317188000027\n",
      "Iteration 584: loss = 0.7790861792454495\n",
      "Iteration 585: loss = 0.7782442467322641\n",
      "Iteration 586: loss = 0.7774058965785025\n",
      "Iteration 587: loss = 0.7765711042788762\n",
      "Iteration 588: loss = 0.7757398455083799\n",
      "Iteration 589: loss = 0.7749120961255441\n",
      "Iteration 590: loss = 0.774087832175298\n",
      "Iteration 591: loss = 0.7732670298914273\n",
      "Iteration 592: loss = 0.7724496656986511\n",
      "Iteration 593: loss = 0.7716357162142956\n",
      "Iteration 594: loss = 0.77082515824959\n",
      "Iteration 595: loss = 0.7700179688105981\n",
      "Iteration 596: loss = 0.7692141250988068\n",
      "Iteration 597: loss = 0.7684136045113914\n",
      "Iteration 598: loss = 0.7676163846411982\n",
      "Iteration 599: loss = 0.7668224432764655\n",
      "Iteration 600: loss = 0.7660317584003262\n",
      "Iteration 601: loss = 0.765244308190125\n",
      "Iteration 602: loss = 0.764460071016597\n",
      "Iteration 603: loss = 0.7636790254429362\n",
      "Iteration 604: loss = 0.7629011502238021\n",
      "Iteration 605: loss = 0.7621264243042982\n",
      "Iteration 606: loss = 0.7613548268189582\n",
      "Iteration 607: loss = 0.7605863370907775\n",
      "Iteration 608: loss = 0.7598209346303122\n",
      "Iteration 609: loss = 0.7590585991348779\n",
      "Iteration 610: loss = 0.7582993104878675\n",
      "Iteration 611: loss = 0.7575430487582019\n",
      "Iteration 612: loss = 0.7567897941999282\n",
      "Iteration 613: loss = 0.7560395272519612\n",
      "Iteration 614: loss = 0.7552922285379844\n",
      "Iteration 615: loss = 0.7545478788664827\n",
      "Iteration 616: loss = 0.7538064592309119\n",
      "Iteration 617: loss = 0.7530679508099823\n",
      "Iteration 618: loss = 0.7523323349680291\n",
      "Iteration 619: loss = 0.7515995932554521\n",
      "Iteration 620: loss = 0.7508697074091928\n",
      "Iteration 621: loss = 0.7501426593532111\n",
      "Iteration 622: loss = 0.749418431198935\n",
      "Iteration 623: loss = 0.7486970052456425\n",
      "Iteration 624: loss = 0.7479783639807418\n",
      "Iteration 625: loss = 0.7472624900799086\n",
      "Iteration 626: loss = 0.746549366407055\n",
      "Iteration 627: loss = 0.7458389760140871\n",
      "Iteration 628: loss = 0.7451313021404239\n",
      "Iteration 629: loss = 0.7444263282122524\n",
      "Iteration 630: loss = 0.7437240378414889\n",
      "Iteration 631: loss = 0.743024414824432\n",
      "Iteration 632: loss = 0.7423274431400863\n",
      "Iteration 633: loss = 0.7416331069481472\n",
      "Iteration 634: loss = 0.7409413905866414\n",
      "Iteration 635: loss = 0.7402522785692183\n",
      "Iteration 636: loss = 0.7395657555820943\n",
      "Iteration 637: loss = 0.738881806480665\n",
      "Iteration 638: loss = 0.7382004162857801\n",
      "Iteration 639: loss = 0.7375215701797168\n",
      "Iteration 640: loss = 0.7368452535018499\n",
      "Iteration 641: loss = 0.7361714517440591\n",
      "Iteration 642: loss = 0.7355001505458856\n",
      "Iteration 643: loss = 0.7348313356894691\n",
      "Iteration 644: loss = 0.7341649930942958\n",
      "Iteration 645: loss = 0.7335011088117852\n",
      "Iteration 646: loss = 0.7328396690197494\n",
      "Iteration 647: loss = 0.7321806600167513\n",
      "Iteration 648: loss = 0.7315240682164031\n",
      "Iteration 649: loss = 0.7308698801416235\n",
      "Iteration 650: loss = 0.730218082418896\n",
      "Iteration 651: loss = 0.7295686617725516\n",
      "Iteration 652: loss = 0.7289216050191072\n",
      "Iteration 653: loss = 0.7282768990616822\n",
      "Iteration 654: loss = 0.7276345308845312\n",
      "Iteration 655: loss = 0.7269944875476998\n",
      "Iteration 656: loss = 0.7263567561818409\n",
      "Iteration 657: loss = 0.7257213239832059\n",
      "Iteration 658: loss = 0.7250881782088325\n",
      "Iteration 659: loss = 0.7244573061719439\n",
      "Iteration 660: loss = 0.723828695237582\n",
      "Iteration 661: loss = 0.7232023328184848\n",
      "Iteration 662: loss = 0.7225782063712232\n",
      "Iteration 663: loss = 0.7219563033926146\n",
      "Iteration 664: loss = 0.7213366114164179\n",
      "Iteration 665: loss = 0.7207191180103251\n",
      "Iteration 666: loss = 0.720103810773263\n",
      "Iteration 667: loss = 0.7194906773330065\n",
      "Iteration 668: loss = 0.7188797053441189\n",
      "Iteration 669: loss = 0.7182708824862185\n",
      "Iteration 670: loss = 0.7176641964625905\n",
      "Iteration 671: loss = 0.7170596349991372\n",
      "Iteration 672: loss = 0.7164571858436759\n",
      "Iteration 673: loss = 0.7158568367655912\n",
      "Iteration 674: loss = 0.7152585755558389\n",
      "Iteration 675: loss = 0.7146623900273006\n",
      "Iteration 676: loss = 0.7140682680154968\n",
      "Iteration 677: loss = 0.7134761973796416\n",
      "Iteration 678: loss = 0.7128861660040502\n",
      "Iteration 679: loss = 0.7122981617998785\n",
      "Iteration 680: loss = 0.711712172707193\n",
      "Iteration 681: loss = 0.7111281866973652\n",
      "Iteration 682: loss = 0.7105461917757584\n",
      "Iteration 683: loss = 0.7099661759847209\n",
      "Iteration 684: loss = 0.7093881274068382\n",
      "Iteration 685: loss = 0.7088120341684485\n",
      "Iteration 686: loss = 0.7082378844433842\n",
      "Iteration 687: loss = 0.7076656664569305\n",
      "Iteration 688: loss = 0.7070953684899595\n",
      "Iteration 689: loss = 0.7065269788832338\n",
      "Iteration 690: loss = 0.7059604860418365\n",
      "Iteration 691: loss = 0.7053958784397074\n",
      "Iteration 692: loss = 0.7048331446242575\n",
      "Iteration 693: loss = 0.7042722732210299\n",
      "Iteration 694: loss = 0.703713252938377\n",
      "Iteration 695: loss = 0.7031560725721331\n",
      "Iteration 696: loss = 0.7026007210102452\n",
      "Iteration 697: loss = 0.7020471872373392\n",
      "Iteration 698: loss = 0.7014954603391951\n",
      "Iteration 699: loss = 0.7009455295071015\n",
      "Iteration 700: loss = 0.7003973840420685\n",
      "Iteration 701: loss = 0.6998510133588768\n",
      "Iteration 702: loss = 0.6993064069899366\n",
      "Iteration 703: loss = 0.6987635545889366\n",
      "Iteration 704: loss = 0.6982224459342731\n",
      "Iteration 705: loss = 0.6976830709322291\n",
      "Iteration 706: loss = 0.6971454196198952\n",
      "Iteration 707: loss = 0.6966094821678162\n",
      "Iteration 708: loss = 0.6960752488823572\n",
      "Iteration 709: loss = 0.6955427102077665\n",
      "Iteration 710: loss = 0.6950118567279332\n",
      "Iteration 711: loss = 0.694482679167837\n",
      "Iteration 712: loss = 0.6939551683946702\n",
      "Iteration 713: loss = 0.6934293154186398\n",
      "Iteration 714: loss = 0.6929051113934376\n",
      "Iteration 715: loss = 0.6923825476163821\n",
      "Iteration 716: loss = 0.6918616155282258\n",
      "Iteration 717: loss = 0.6913423067126322\n",
      "Iteration 718: loss = 0.6908246128953215\n",
      "Iteration 719: loss = 0.6903085259428903\n",
      "Iteration 720: loss = 0.6897940378613087\n",
      "Iteration 721: loss = 0.6892811407940944\n",
      "Iteration 722: loss = 0.6887698270201821\n",
      "Iteration 723: loss = 0.6882600889514854\n",
      "Iteration 724: loss = 0.6877519191301625\n",
      "Iteration 725: loss = 0.6872453102256031\n",
      "Iteration 726: loss = 0.686740255031136\n",
      "Iteration 727: loss = 0.6862367464604813\n",
      "Iteration 728: loss = 0.685734777543956\n",
      "Iteration 729: loss = 0.6852343414244442\n",
      "Iteration 730: loss = 0.6847354313531615\n",
      "Iteration 731: loss = 0.6842380406852154\n",
      "Iteration 732: loss = 0.6837421628749913\n",
      "Iteration 733: loss = 0.6832477914713793\n",
      "Iteration 734: loss = 0.682754920112866\n",
      "Iteration 735: loss = 0.6822635425225078\n",
      "Iteration 736: loss = 0.6817736525028126\n",
      "Iteration 737: loss = 0.6812852439305553\n",
      "Iteration 738: loss = 0.6807983107515401\n",
      "Iteration 739: loss = 0.6803128469753495\n",
      "Iteration 740: loss = 0.679828846670091\n",
      "Iteration 741: loss = 0.6793463039571722\n",
      "Iteration 742: loss = 0.6788652130061316\n",
      "Iteration 743: loss = 0.6783855680295423\n",
      "Iteration 744: loss = 0.6779073632780173\n",
      "Iteration 745: loss = 0.6774305930353408\n",
      "Iteration 746: loss = 0.6769552516137401\n",
      "Iteration 747: loss = 0.6764813333493346\n",
      "Iteration 748: loss = 0.6760088325977611\n",
      "Iteration 749: loss = 0.6755377437300146\n",
      "Iteration 750: loss = 0.6750680611285079\n",
      "Iteration 751: loss = 0.6745997791833724\n",
      "Iteration 752: loss = 0.6741328922890103\n",
      "Iteration 753: loss = 0.673667394840912\n",
      "Iteration 754: loss = 0.6732032812327439\n",
      "Iteration 755: loss = 0.6727405458537253\n",
      "Iteration 756: loss = 0.6722791830862803\n",
      "Iteration 757: loss = 0.6718191873039894\n",
      "Iteration 758: loss = 0.671360552869826\n",
      "Iteration 759: loss = 0.6709032741346814\n",
      "Iteration 760: loss = 0.6704473454361827\n",
      "Iteration 761: loss = 0.6699927610977818\n",
      "Iteration 762: loss = 0.6695395154281292\n",
      "Iteration 763: loss = 0.6690876027207043\n",
      "Iteration 764: loss = 0.6686370172537104\n",
      "Iteration 765: loss = 0.6681877532902072\n",
      "Iteration 766: loss = 0.6677398050784811\n",
      "Iteration 767: loss = 0.6672931668526328\n",
      "Iteration 768: loss = 0.6668478328333697\n",
      "Iteration 769: loss = 0.6664037972289893\n",
      "Iteration 770: loss = 0.6659610542365393\n",
      "Iteration 771: loss = 0.6655195980431377\n",
      "Iteration 772: loss = 0.6650794228274355\n",
      "Iteration 773: loss = 0.6646405227612119\n",
      "Iteration 774: loss = 0.6642028920110827\n",
      "Iteration 775: loss = 0.6637665247403064\n",
      "Iteration 776: loss = 0.6633314151106767\n",
      "Iteration 777: loss = 0.6628975572844858\n",
      "Iteration 778: loss = 0.6624649454265418\n",
      "Iteration 779: loss = 0.6620335737062311\n",
      "Iteration 780: loss = 0.6616034362996112\n",
      "Iteration 781: loss = 0.6611745273915247\n",
      "Iteration 782: loss = 0.6607468411777143\n",
      "Iteration 783: loss = 0.6603203718669434\n",
      "Iteration 784: loss = 0.6598951136830933\n",
      "Iteration 785: loss = 0.6594710608672499\n",
      "Iteration 786: loss = 0.6590482076797454\n",
      "Iteration 787: loss = 0.6586265484021714\n",
      "Iteration 788: loss = 0.6582060773393407\n",
      "Iteration 789: loss = 0.657786788821195\n",
      "Iteration 790: loss = 0.6573686772046559\n",
      "Iteration 791: loss = 0.6569517368754099\n",
      "Iteration 792: loss = 0.6565359622496203\n",
      "Iteration 793: loss = 0.6561213477755751\n",
      "Iteration 794: loss = 0.6557078879352475\n",
      "Iteration 795: loss = 0.6552955772457856\n",
      "Iteration 796: loss = 0.6548844102609154\n",
      "Iteration 797: loss = 0.6544743815722643\n",
      "Iteration 798: loss = 0.6540654858106022\n",
      "Iteration 799: loss = 0.653657717646994\n",
      "Iteration 800: loss = 0.6532510717938792\n",
      "Iteration 801: loss = 0.6528455430060593\n",
      "Iteration 802: loss = 0.6524411260816165\n",
      "Iteration 803: loss = 0.6520378158627451\n",
      "Iteration 804: loss = 0.651635607236517\n",
      "Iteration 805: loss = 0.6512344951355714\n",
      "Iteration 806: loss = 0.6508344745387368\n",
      "Iteration 807: loss = 0.6504355404715936\n",
      "Iteration 808: loss = 0.650037688006973\n",
      "Iteration 809: loss = 0.649640912265403\n",
      "Iteration 810: loss = 0.6492452084155063\n",
      "Iteration 811: loss = 0.6488505716743469\n",
      "Iteration 812: loss = 0.6484569973077428\n",
      "Iteration 813: loss = 0.6480644806305376\n",
      "Iteration 814: loss = 0.6476730170068447\n",
      "Iteration 815: loss = 0.6472826018502585\n",
      "Iteration 816: loss = 0.6468932306240488\n",
      "Iteration 817: loss = 0.6465048988413306\n",
      "Iteration 818: loss = 0.6461176020652222\n",
      "Iteration 819: loss = 0.6457313359089869\n",
      "Iteration 820: loss = 0.6453460960361649\n",
      "Iteration 821: loss = 0.6449618781607015\n",
      "Iteration 822: loss = 0.6445786780470608\n",
      "Iteration 823: loss = 0.644196491510343\n",
      "Iteration 824: loss = 0.6438153144163883\n",
      "Iteration 825: loss = 0.6434351426818845\n",
      "Iteration 826: loss = 0.643055972274464\n",
      "Iteration 827: loss = 0.6426777992127976\n",
      "Iteration 828: loss = 0.6423006195666827\n",
      "Iteration 829: loss = 0.6419244294571209\n",
      "Iteration 830: loss = 0.6415492250563871\n",
      "Iteration 831: loss = 0.6411750025880859\n",
      "Iteration 832: loss = 0.6408017583271916\n",
      "Iteration 833: loss = 0.6404294886000723\n",
      "Iteration 834: loss = 0.640058189784493\n",
      "Iteration 835: loss = 0.6396878583095947\n",
      "Iteration 836: loss = 0.6393184906558459\n",
      "Iteration 837: loss = 0.6389500833549668\n",
      "Iteration 838: loss = 0.6385826329898179\n",
      "Iteration 839: loss = 0.638216136194255\n",
      "Iteration 840: loss = 0.6378505896529459\n",
      "Iteration 841: loss = 0.6374859901011463\n",
      "Iteration 842: loss = 0.637122334324433\n",
      "Iteration 843: loss = 0.6367596191583914\n",
      "Iteration 844: loss = 0.6363978414882593\n",
      "Iteration 845: loss = 0.6360369982485236\n",
      "Iteration 846: loss = 0.635677086422467\n",
      "Iteration 847: loss = 0.6353181030416699\n",
      "Iteration 848: loss = 0.6349600451854623\n",
      "Iteration 849: loss = 0.6346029099803319\n",
      "Iteration 850: loss = 0.6342466945992846\n",
      "Iteration 851: loss = 0.6338913962611634\n",
      "Iteration 852: loss = 0.6335370122299253\n",
      "Iteration 853: loss = 0.6331835398138773\n",
      "Iteration 854: loss = 0.6328309763648827\n",
      "Iteration 855: loss = 0.6324793192775305\n",
      "Iteration 856: loss = 0.6321285659882805\n",
      "Iteration 857: loss = 0.6317787139745816\n",
      "Iteration 858: loss = 0.631429760753978\n",
      "Iteration 859: loss = 0.6310817038831946\n",
      "Iteration 860: loss = 0.6307345409572188\n",
      "Iteration 861: loss = 0.6303882696083801\n",
      "Iteration 862: loss = 0.6300428875054285\n",
      "Iteration 863: loss = 0.6296983923526263\n",
      "Iteration 864: loss = 0.629354781888851\n",
      "Iteration 865: loss = 0.629012053886718\n",
      "Iteration 866: loss = 0.6286702061517319\n",
      "Iteration 867: loss = 0.628329236521462\n",
      "Iteration 868: loss = 0.6279891428647602\n",
      "Iteration 869: loss = 0.6276499230810123\n",
      "Iteration 870: loss = 0.6273115750994377\n",
      "Iteration 871: loss = 0.6269740968784339\n",
      "Iteration 872: loss = 0.6266374864049722\n",
      "Iteration 873: loss = 0.6263017416940455\n",
      "Iteration 874: loss = 0.6259668607881714\n",
      "Iteration 875: loss = 0.6256328417569484\n",
      "Iteration 876: loss = 0.6252996826966685\n",
      "Iteration 877: loss = 0.6249673817299811\n",
      "Iteration 878: loss = 0.624635937005615\n",
      "Iteration 879: loss = 0.6243053466981423\n",
      "Iteration 880: loss = 0.6239756090077956\n",
      "Iteration 881: loss = 0.6236467221603214\n",
      "Iteration 882: loss = 0.6233186844068749\n",
      "Iteration 883: loss = 0.6229914940239438\n",
      "Iteration 884: loss = 0.6226651493132981\n",
      "Iteration 885: loss = 0.622339648601955\n",
      "Iteration 886: loss = 0.6220149902421606\n",
      "Iteration 887: loss = 0.6216911726113661\n",
      "Iteration 888: loss = 0.6213681941122073\n",
      "Iteration 889: loss = 0.6210460531724652\n",
      "Iteration 890: loss = 0.6207247482450097\n",
      "Iteration 891: loss = 0.6204042778077118\n",
      "Iteration 892: loss = 0.620084640363322\n",
      "Iteration 893: loss = 0.6197658344393041\n",
      "Iteration 894: loss = 0.6194478585876224\n",
      "Iteration 895: loss = 0.6191307113844702\n",
      "Iteration 896: loss = 0.6188143914299395\n",
      "Iteration 897: loss = 0.6184988973476262\n",
      "Iteration 898: loss = 0.6181842277841674\n",
      "Iteration 899: loss = 0.6178703814087051\n",
      "Iteration 900: loss = 0.6175573569122789\n",
      "Iteration 901: loss = 0.6172451530071443\n",
      "Iteration 902: loss = 0.6169337684260146\n",
      "Iteration 903: loss = 0.6166232019212273\n",
      "Iteration 904: loss = 0.6163134522638405\n",
      "Iteration 905: loss = 0.6160045182426512\n",
      "Iteration 906: loss = 0.6156963986631481\n",
      "Iteration 907: loss = 0.6153890923463898\n",
      "Iteration 908: loss = 0.6150825981278257\n",
      "Iteration 909: loss = 0.6147769148560416\n",
      "Iteration 910: loss = 0.6144720413914582\n",
      "Iteration 911: loss = 0.6141679766049631\n",
      "Iteration 912: loss = 0.6138647193764928\n",
      "Iteration 913: loss = 0.6135622685935662\n",
      "Iteration 914: loss = 0.6132606231497663\n",
      "Iteration 915: loss = 0.6129597819431869\n",
      "Iteration 916: loss = 0.6126597438748316\n",
      "Iteration 917: loss = 0.6123605078469839\n",
      "Iteration 918: loss = 0.6120620727615433\n",
      "Iteration 919: loss = 0.6117644375183376\n",
      "Iteration 920: loss = 0.6114676010134099\n",
      "Iteration 921: loss = 0.6111715621372897\n",
      "Iteration 922: loss = 0.6108763197732561\n",
      "Iteration 923: loss = 0.6105818727955901\n",
      "Iteration 924: loss = 0.61028822006783\n",
      "Iteration 925: loss = 0.6099953604410344\n",
      "Iteration 926: loss = 0.6097032927520576\n",
      "Iteration 927: loss = 0.6094120158218459\n",
      "Iteration 928: loss = 0.609121528453764\n",
      "Iteration 929: loss = 0.6088318294319581\n",
      "Iteration 930: loss = 0.6085429175197623\n",
      "Iteration 931: loss = 0.6082547914581592\n",
      "Iteration 932: loss = 0.6079674499643017\n",
      "Iteration 933: loss = 0.6076808917301043\n",
      "Iteration 934: loss = 0.6073951154209118\n",
      "Iteration 935: loss = 0.6071101196742524\n",
      "Iteration 936: loss = 0.6068259030986837\n",
      "Iteration 937: loss = 0.606542464272737\n",
      "Iteration 938: loss = 0.6062598017439683\n",
      "Iteration 939: loss = 0.6059779140281166\n",
      "Iteration 940: loss = 0.6056967996083775\n",
      "Iteration 941: loss = 0.6054164569347955\n",
      "Iteration 942: loss = 0.605136884423774\n",
      "Iteration 943: loss = 0.6048580804577091\n",
      "Iteration 944: loss = 0.6045800433847388\n",
      "Iteration 945: loss = 0.6043027715186193\n",
      "Iteration 946: loss = 0.6040262631387119\n",
      "Iteration 947: loss = 0.6037505164900854\n",
      "Iteration 948: loss = 0.6034755297837289\n",
      "Iteration 949: loss = 0.6032013011968652\n",
      "Iteration 950: loss = 0.6029278288733625\n",
      "Iteration 951: loss = 0.6026551109242307\n",
      "Iteration 952: loss = 0.602383145428201\n",
      "Iteration 953: loss = 0.6021119304323733\n",
      "Iteration 954: loss = 0.6018414639529235\n",
      "Iteration 955: loss = 0.6015717439758632\n",
      "Iteration 956: loss = 0.601302768457835\n",
      "Iteration 957: loss = 0.6010345353269358\n",
      "Iteration 958: loss = 0.6007670424835606\n",
      "Iteration 959: loss = 0.6005002878012459\n",
      "Iteration 960: loss = 0.6002342691275164\n",
      "Iteration 961: loss = 0.5999689842847077\n",
      "Iteration 962: loss = 0.5997044310707711\n",
      "Iteration 963: loss = 0.5994406072600423\n",
      "Iteration 964: loss = 0.5991775106039664\n",
      "Iteration 965: loss = 0.5989151388317778\n",
      "Iteration 966: loss = 0.5986534896511201\n",
      "Iteration 967: loss = 0.5983925607486084\n",
      "Iteration 968: loss = 0.5981323497903263\n",
      "Iteration 969: loss = 0.5978728544222557\n",
      "Iteration 970: loss = 0.597614072270637\n",
      "Iteration 971: loss = 0.5973560009422619\n",
      "Iteration 972: loss = 0.597098638024699\n",
      "Iteration 973: loss = 0.5968419810864498\n",
      "Iteration 974: loss = 0.5965860276770476\n",
      "Iteration 975: loss = 0.596330775327097\n",
      "Iteration 976: loss = 0.5960762215482608\n",
      "Iteration 977: loss = 0.5958223638332011\n",
      "Iteration 978: loss = 0.5955691996554845\n",
      "Iteration 979: loss = 0.5953167264694539\n",
      "Iteration 980: loss = 0.5950649417100777\n",
      "Iteration 980: loss = 0.5950649417100777\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Add layers to the neural network\n",
    "model = Sequential()\n",
    "model.add(LinearLayer(input_size=784, output_size=350))\n",
    "model.add(SigmoidLayer())\n",
    "model.add(LinearLayer(input_size=10, output_size=10))\n",
    "model.add(SigmoidLayer())\n",
    "\n",
    "# Define the loss function\n",
    "loss = CrossEntropyLoss()\n",
    "\n",
    "\n",
    "model.train(X_train,y_train,learning_rate = learning_rate,loss_print_count=1)\n",
    "\n",
    "\n",
    "\n",
    "# Evaluate the neural network on the test set\n",
    "y_test_pred = model.predict(X_test)\n",
    "test_loss = loss.forward(y_test_pred, y_test)\n",
    "print(f\"Test Loss={test_loss:.6f}\")\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "# wait = 0\n",
    "\n",
    "# for epoch in range(epochs):\n",
    "#     # Forward pass\n",
    "#     y_pred = model.forward(X_train)\n",
    "#     train_loss = loss.forward(y_pred, y_train)\n",
    "\n",
    "#     # Backward pass\n",
    "#     error = loss.backward()\n",
    "#     model.backward(error, learning_rate)\n",
    "\n",
    "#     # Compute validation loss\n",
    "#     y_val_pred = model.predict(X_val)\n",
    "#     val_loss = loss.forward(y_val_pred, y_val)\n",
    "\n",
    "#     # Check if validation loss has improved\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
